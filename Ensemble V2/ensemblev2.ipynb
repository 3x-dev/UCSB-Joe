{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f84c0b2e1b0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 1: Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import joblib\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Set seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the regular expression for removing special characters and punctuation\n",
    "special_char_re = re.compile(r'[^a-zA-Z0-9\\s]')\n",
    "\n",
    "# Download necessary NLTK resources if not already downloaded\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Predefine the stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans the input text by converting to lowercase, removing special characters and punctuation,\n",
    "    tokenizing, removing stop words, and joining the tokens back into a string.\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and punctuation\n",
    "    text = special_char_re.sub('', text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Join tokens back into a string\n",
    "    cleaned_text = ' '.join(tokens)\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled dataset loaded. Shape: (491, 42)\n",
      "Relevant columns from labeled dataset extracted. Shape: (491, 6)\n",
      "Messages cleaned.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load the labeled dataset and clean it\n",
    "labeled_df = pd.read_csv('Stormfront_labeled.csv')\n",
    "print(\"Labeled dataset loaded. Shape:\", labeled_df.shape)\n",
    "\n",
    "# Extract relevant columns\n",
    "labeled_df = labeled_df[['Message', 'CM', 'AOPV', 'CDACT', 'TI', 'TTBF']]\n",
    "print(\"Relevant columns from labeled dataset extracted. Shape:\", labeled_df.shape)\n",
    "\n",
    "# Clean the messages\n",
    "labeled_df['Message'] = labeled_df['Message'].apply(clean_text)\n",
    "print(\"Messages cleaned.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unlabeled dataset loaded and sampled. Shape: (5000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load the unlabeled dataset in chunks and clean it\n",
    "chunk_size = 100000  # Adjust chunk size as needed\n",
    "target_unlabeled_size = 5000  # Increased target size for testing\n",
    "chunks = []\n",
    "\n",
    "for chunk in pd.read_csv('cleanposts.csv', chunksize=chunk_size):\n",
    "    # Filter out non-English messages\n",
    "    chunk = chunk[chunk['lang'] == 'en']\n",
    "    chunks.append(chunk[['cleanmessage']].rename(columns={'cleanmessage': 'Message'}))\n",
    "    \n",
    "    # Check if we have reached the target size\n",
    "    if sum(len(c) for c in chunks) >= target_unlabeled_size:\n",
    "        break\n",
    "\n",
    "unlabeled_df = pd.concat(chunks).head(target_unlabeled_size)\n",
    "unlabeled_df['Message'] = unlabeled_df['Message'].apply(clean_text)\n",
    "print(\"Unlabeled dataset loaded and sampled. Shape:\", unlabeled_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/joblib/externals/loky/backend/fork_exec.py:43: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid = os.fork()\n",
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:778: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aryan/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 765, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: _BaseScorer.__call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_df=0.85, max_features=5000, min_df=5, ngram_range=(1, 1); total time=   0.1s\n",
      "[CV] END max_df=0.85, max_features=5000, min_df=5, ngram_range=(1, 1); total time=   0.1s\n",
      "[CV] END max_df=0.85, max_features=5000, min_df=5, ngram_range=(1, 1); total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:778: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aryan/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 765, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: _BaseScorer.__call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:778: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aryan/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 765, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: _BaseScorer.__call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_df=0.85, max_features=10000, min_df=5, ngram_range=(1, 1); total time=   0.2s\n",
      "[CV] END max_df=0.85, max_features=10000, min_df=5, ngram_range=(1, 1); total time=   0.2s\n",
      "[CV] END max_df=0.85, max_features=10000, min_df=5, ngram_range=(1, 1); total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_df=0.85, max_features=10000, min_df=5, ngram_range=(1, 2); total time=   0.2s\n",
      "[CV] END max_df=0.85, max_features=10000, min_df=5, ngram_range=(1, 2); total time=   0.2s\n",
      "[CV] END max_df=0.85, max_features=10000, min_df=5, ngram_range=(1, 2); total time=   0.2s\n",
      "[CV] END max_df=0.85, max_features=10000, min_df=10, ngram_range=(1, 1); total time=   0.1s\n",
      "[CV] END max_df=0.85, max_features=10000, min_df=10, ngram_range=(1, 1); total time=   0.1s\n",
      "[CV] END max_df=0.85, max_features=10000, min_df=10, ngram_range=(1, 1); total time=   0.0s\n",
      "[CV] END max_df=0.85, max_features=5000, min_df=5, ngram_range=(1, 2); total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:778: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aryan/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 765, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: _BaseScorer.__call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_df=0.85, max_features=10000, min_df=10, ngram_range=(1, 2); total time=   0.2s\n",
      "[CV] END max_df=0.85, max_features=10000, min_df=10, ngram_range=(1, 2); total time=   0.2s\n",
      "[CV] END max_df=0.85, max_features=10000, min_df=10, ngram_range=(1, 2); total time=   0.2s\n",
      "[CV] END max_df=0.85, max_features=20000, min_df=5, ngram_range=(1, 1); total time=   0.0s\n",
      "[CV] END max_df=0.85, max_features=20000, min_df=5, ngram_range=(1, 1); total time=   0.1s\n",
      "[CV] END max_df=0.85, max_features=5000, min_df=5, ngram_range=(1, 2); total time=   0.2s\n",
      "[CV] END max_df=0.85, max_features=20000, min_df=5, ngram_range=(1, 1); total time=   0.1s\n",
      "[CV] END max_df=0.85, max_features=5000, min_df=5, ngram_range=(1, 2); total time=   0.2s\n",
      "[CV] END max_df=0.85, max_features=20000, min_df=10, ngram_range=(1, 1); total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:778: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aryan/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 765, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: _BaseScorer.__call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:778: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aryan/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 765, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: _BaseScorer.__call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:778: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aryan/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 765, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: _BaseScorer.__call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_df=0.85, max_features=20000, min_df=10, ngram_range=(1, 1); total time=   0.1s\n",
      "[CV] END max_df=0.85, max_features=20000, min_df=5, ngram_range=(1, 2); total time=   0.2s\n",
      "[CV] END max_df=0.85, max_features=20000, min_df=10, ngram_range=(1, 1); total time=   0.1s\n",
      "[CV] END max_df=0.85, max_features=20000, min_df=5, ngram_range=(1, 2); total time=   0.2s\n",
      "[CV] END max_df=0.85, max_features=5000, min_df=10, ngram_range=(1, 1); total time=   0.1s\n",
      "[CV] END max_df=0.85, max_features=20000, min_df=5, ngram_range=(1, 2); total time=   0.2s\n",
      "[CV] END max_df=0.9, max_features=5000, min_df=5, ngram_range=(1, 1); total time=   0.1s\n",
      "[CV] END max_df=0.9, max_features=5000, min_df=5, ngram_range=(1, 1); total time=   0.1s\n",
      "[CV] END max_df=0.9, max_features=5000, min_df=5, ngram_range=(1, 1); total time=   0.1s\n",
      "[CV] END max_df=0.85, max_features=20000, min_df=10, ngram_range=(1, 2); total time=   0.3s\n",
      "[CV] END max_df=0.85, max_features=20000, min_df=10, ngram_range=(1, 2); total time=   0.3s\n",
      "[CV] END max_df=0.9, max_features=5000, min_df=10, ngram_range=(1, 1); total time=   0.1s\n",
      "[CV] END max_df=0.85, max_features=20000, min_df=10, ngram_range=(1, 2); total time=   0.3s\n",
      "[CV] END max_df=0.9, max_features=5000, min_df=10, ngram_range=(1, 1); total time=   0.1s\n",
      "[CV] END max_df=0.9, max_features=5000, min_df=10, ngram_range=(1, 1); total time=   0.1s\n",
      "[CV] END max_df=0.9, max_features=5000, min_df=5, ngram_range=(1, 2); total time=   0.2s\n",
      "[CV] END max_df=0.9, max_features=5000, min_df=5, ngram_range=(1, 2); total time=   0.2s\n",
      "[CV] END max_df=0.9, max_features=5000, min_df=5, ngram_range=(1, 2); total time=   0.2s\n",
      "[CV] END max_df=0.85, max_features=5000, min_df=10, ngram_range=(1, 1); total time=   0.1s\n",
      "[CV] END max_df=0.9, max_features=10000, min_df=5, ngram_range=(1, 1); total time=   0.0s\n",
      "[CV] END max_df=0.9, max_features=10000, min_df=5, ngram_range=(1, 1); total time=   0.0s\n",
      "[CV] END max_df=0.85, max_features=5000, min_df=10, ngram_range=(1, 1); total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:778: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aryan/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 765, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: _BaseScorer.__call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:778: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aryan/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 765, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: _BaseScorer.__call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_df=0.9, max_features=10000, min_df=5, ngram_range=(1, 1); total time=   0.1s\n",
      "[CV] END max_df=0.9, max_features=5000, min_df=10, ngram_range=(1, 2); total time=   0.2s\n",
      "[CV] END max_df=0.9, max_features=5000, min_df=10, ngram_range=(1, 2); total time=   0.2s\n",
      "[CV] END max_df=0.9, max_features=5000, min_df=10, ngram_range=(1, 2); total time=   0.2s\n",
      "[CV] END max_df=0.9, max_features=10000, min_df=10, ngram_range=(1, 1); total time=   0.1s\n",
      "[CV] END max_df=0.9, max_features=10000, min_df=10, ngram_range=(1, 1); total time=   0.1s\n",
      "[CV] END max_df=0.9, max_features=10000, min_df=10, ngram_range=(1, 1); total time=   0.1s\n",
      "[CV] END max_df=0.9, max_features=10000, min_df=5, ngram_range=(1, 2); total time=   0.3s\n",
      "[CV] END max_df=0.9, max_features=20000, min_df=5, ngram_range=(1, 1); total time=   0.0s\n",
      "[CV] END max_df=0.85, max_features=5000, min_df=10, ngram_range=(1, 2); total time=   0.3s\n",
      "[CV] END max_df=0.9, max_features=20000, min_df=5, ngram_range=(1, 1); total time=   0.1s\n",
      "[CV] END max_df=0.9, max_features=10000, min_df=5, ngram_range=(1, 2); total time=   0.3s\n",
      "[CV] END max_df=0.9, max_features=10000, min_df=5, ngram_range=(1, 2); total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:778: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aryan/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 765, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: _BaseScorer.__call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:778: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aryan/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 765, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: _BaseScorer.__call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:778: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aryan/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 765, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: _BaseScorer.__call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_df=0.9, max_features=20000, min_df=5, ngram_range=(1, 1); total time=   0.1s\n",
      "[CV] END max_df=0.9, max_features=20000, min_df=10, ngram_range=(1, 1); total time=   0.1s\n",
      "[CV] END max_df=0.85, max_features=5000, min_df=10, ngram_range=(1, 2); total time=   0.2s\n",
      "[CV] END max_df=0.9, max_features=10000, min_df=10, ngram_range=(1, 2); total time=   0.2s\n",
      "[CV] END max_df=0.9, max_features=20000, min_df=10, ngram_range=(1, 1); total time=   0.1s\n",
      "[CV] END max_df=0.9, max_features=20000, min_df=10, ngram_range=(1, 1); total time=   0.1s\n",
      "[CV] END max_df=0.9, max_features=10000, min_df=10, ngram_range=(1, 2); total time=   0.2s\n",
      "[CV] END max_df=0.9, max_features=10000, min_df=10, ngram_range=(1, 2); total time=   0.2s\n",
      "[CV] END max_df=0.9, max_features=20000, min_df=5, ngram_range=(1, 2); total time=   0.2s\n",
      "[CV] END max_df=0.9, max_features=20000, min_df=5, ngram_range=(1, 2); total time=   0.2s\n",
      "[CV] END max_df=0.9, max_features=20000, min_df=5, ngram_range=(1, 2); total time=   0.2s\n",
      "[CV] END max_df=0.85, max_features=5000, min_df=10, ngram_range=(1, 2); total time=   0.2s\n",
      "[CV] END max_df=0.9, max_features=20000, min_df=10, ngram_range=(1, 2); total time=   0.1s\n",
      "[CV] END max_df=0.9, max_features=20000, min_df=10, ngram_range=(1, 2); total time=   0.1s\n",
      "[CV] END max_df=0.9, max_features=20000, min_df=10, ngram_range=(1, 2); total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: TF-IDF Vectorization with hyperparameter tuning\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Custom scoring function for TF-IDF Vectorizer\n",
    "def tfidf_scorer(estimator, X):\n",
    "    X_transformed = estimator.transform(X)\n",
    "    score = np.mean(cosine_similarity(X_transformed))\n",
    "    return score\n",
    "\n",
    "# Define a function to manually perform TF-IDF Vectorization with hyperparameter tuning\n",
    "def tune_tfidf_vectorizer(texts):\n",
    "    param_grid = {\n",
    "        'max_features': [5000, 10000, 20000],\n",
    "        'ngram_range': [(1, 1), (1, 2)],\n",
    "        'min_df': [5, 10],\n",
    "        'max_df': [0.85, 0.9]\n",
    "    }\n",
    "    tfidf = TfidfVectorizer()\n",
    "    grid_search = GridSearchCV(tfidf, param_grid, cv=3, n_jobs=-1, verbose=2, scoring=make_scorer(tfidf_scorer))\n",
    "    grid_search.fit(texts)  # Only fit on texts\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "# Perform TF-IDF Vectorization with tuning on the labeled dataset\n",
    "tfidf_vectorizer = tune_tfidf_vectorizer(labeled_df['Message'])\n",
    "tfidf_features_labeled = tfidf_vectorizer.fit_transform(labeled_df['Message']).toarray()\n",
    "tfidf_features_unlabeled = tfidf_vectorizer.transform(unlabeled_df['Message']).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Extracting BERT Features: 100%|██████████| 16/16 [00:49<00:00,  3.12s/it]\n",
      "Extracting BERT Features: 100%|██████████| 157/157 [07:07<00:00,  2.72s/it]\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: BERT Feature Extraction\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "distilbert_model = AutoModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def get_bert_features(texts, tokenizer, model, batch_size=32):\n",
    "    features = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Extracting BERT Features\"):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True, max_length=256)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        batch_features = outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "        features.append(batch_features)\n",
    "    return np.vstack(features)\n",
    "\n",
    "# Extract BERT features\n",
    "bert_features_labeled = get_bert_features(labeled_df['Message'].tolist(), tokenizer, distilbert_model)\n",
    "bert_features_unlabeled = get_bert_features(unlabeled_df['Message'].tolist(), tokenizer, distilbert_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (392, 1562), Validation data shape: (99, 1562)\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Combine TF-IDF and BERT features\n",
    "labeled_features = np.hstack([tfidf_features_labeled, bert_features_labeled])\n",
    "unlabeled_features = np.hstack([tfidf_features_unlabeled, bert_features_unlabeled])\n",
    "\n",
    "# Convert to DataFrame for compatibility with model training\n",
    "labeled_features_df = pd.DataFrame(labeled_features)\n",
    "unlabeled_features_df = pd.DataFrame(unlabeled_features)\n",
    "\n",
    "# Add target columns to the labeled features DataFrame\n",
    "labeled_features_df[['CM', 'AOPV', 'CDACT', 'TI', 'TTBF']] = labeled_df[['CM', 'AOPV', 'CDACT', 'TI', 'TTBF']]\n",
    "# Split the labeled data into training and validation sets\n",
    "train_df, val_df = train_test_split(labeled_features_df, test_size=0.2, random_state=42)\n",
    "print(f\"Training data shape: {train_df.shape}, Validation data shape: {val_df.shape}\")\n",
    "\n",
    "# Save train and validation data to disk\n",
    "train_df.to_csv('train_df.csv', index=False)\n",
    "val_df.to_csv('val_df.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Define parameter grids for the models\n",
    "param_grid_rf = {\n",
    "    'estimator__n_estimators': [100, 200, 300, 500, 1000],\n",
    "    'estimator__max_depth': [10, 20, 30, 50],\n",
    "    'estimator__min_samples_split': [2, 5, 10],\n",
    "    'estimator__min_samples_leaf': [1, 2, 4, 8],\n",
    "    'estimator__max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "param_grid_gb = {\n",
    "    'estimator__n_estimators': [100, 200, 300, 500, 1000],\n",
    "    'estimator__max_depth': [3, 5, 7, 9, 12],\n",
    "    'estimator__learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3]\n",
    "}\n",
    "\n",
    "# Initialize the models\n",
    "rf_model = MultiOutputRegressor(RandomForestRegressor(random_state=42))\n",
    "gb_model = MultiOutputRegressor(GradientBoostingRegressor(random_state=42))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting RandomizedSearchCV for RandomForestRegressor...\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/joblib/externals/loky/backend/fork_exec.py:43: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END estimator__max_depth=30, estimator__max_features=sqrt, estimator__min_samples_leaf=4, estimator__min_samples_split=5, estimator__n_estimators=300; total time=  10.8s\n",
      "[CV] END estimator__max_depth=30, estimator__max_features=sqrt, estimator__min_samples_leaf=4, estimator__min_samples_split=5, estimator__n_estimators=300; total time=  10.8s\n",
      "[CV] END estimator__max_depth=30, estimator__max_features=sqrt, estimator__min_samples_leaf=4, estimator__min_samples_split=5, estimator__n_estimators=300; total time=  10.9s\n",
      "[CV] END estimator__max_depth=10, estimator__max_features=log2, estimator__min_samples_leaf=1, estimator__min_samples_split=10, estimator__n_estimators=500; total time=  11.6s\n",
      "[CV] END estimator__max_depth=10, estimator__max_features=log2, estimator__min_samples_leaf=1, estimator__min_samples_split=10, estimator__n_estimators=500; total time=  11.6s\n",
      "[CV] END estimator__max_depth=10, estimator__max_features=log2, estimator__min_samples_leaf=1, estimator__min_samples_split=10, estimator__n_estimators=500; total time=  11.7s\n",
      "[CV] END estimator__max_depth=20, estimator__max_features=sqrt, estimator__min_samples_leaf=8, estimator__min_samples_split=5, estimator__n_estimators=300; total time=   9.1s\n",
      "[CV] END estimator__max_depth=20, estimator__max_features=sqrt, estimator__min_samples_leaf=8, estimator__min_samples_split=5, estimator__n_estimators=300; total time=   9.2s\n",
      "[CV] END estimator__max_depth=20, estimator__max_features=sqrt, estimator__min_samples_leaf=8, estimator__min_samples_split=5, estimator__n_estimators=300; total time=   9.2s\n",
      "[CV] END estimator__max_depth=50, estimator__max_features=sqrt, estimator__min_samples_leaf=8, estimator__min_samples_split=5, estimator__n_estimators=1000; total time=  28.4s\n",
      "[CV] END estimator__max_depth=50, estimator__max_features=sqrt, estimator__min_samples_leaf=8, estimator__min_samples_split=5, estimator__n_estimators=1000; total time=  28.6s\n",
      "[CV] END estimator__max_depth=50, estimator__max_features=sqrt, estimator__min_samples_leaf=8, estimator__min_samples_split=5, estimator__n_estimators=1000; total time=  28.5s\n",
      "[CV] END estimator__max_depth=50, estimator__max_features=sqrt, estimator__min_samples_leaf=2, estimator__min_samples_split=2, estimator__n_estimators=100; total time=   3.9s\n",
      "[CV] END estimator__max_depth=50, estimator__max_features=sqrt, estimator__min_samples_leaf=2, estimator__min_samples_split=2, estimator__n_estimators=100; total time=   3.9s\n",
      "[CV] END estimator__max_depth=50, estimator__max_features=sqrt, estimator__min_samples_leaf=2, estimator__min_samples_split=2, estimator__n_estimators=100; total time=   4.0s\n",
      "[CV] END estimator__max_depth=50, estimator__max_features=sqrt, estimator__min_samples_leaf=4, estimator__min_samples_split=2, estimator__n_estimators=1000; total time=  32.9s\n",
      "[CV] END estimator__max_depth=50, estimator__max_features=sqrt, estimator__min_samples_leaf=4, estimator__min_samples_split=2, estimator__n_estimators=1000; total time=  33.2s\n",
      "[CV] END estimator__max_depth=50, estimator__max_features=sqrt, estimator__min_samples_leaf=4, estimator__min_samples_split=2, estimator__n_estimators=1000; total time=  33.4s\n",
      "[CV] END estimator__max_depth=30, estimator__max_features=sqrt, estimator__min_samples_leaf=2, estimator__min_samples_split=10, estimator__n_estimators=500; total time=  16.8s\n",
      "[CV] END estimator__max_depth=30, estimator__max_features=sqrt, estimator__min_samples_leaf=2, estimator__min_samples_split=10, estimator__n_estimators=500; total time=  17.1s\n",
      "[CV] END estimator__max_depth=30, estimator__max_features=sqrt, estimator__min_samples_leaf=2, estimator__min_samples_split=10, estimator__n_estimators=500; total time=  17.1s\n",
      "[CV] END estimator__max_depth=10, estimator__max_features=log2, estimator__min_samples_leaf=2, estimator__min_samples_split=2, estimator__n_estimators=300; total time=   6.0s\n",
      "[CV] END estimator__max_depth=10, estimator__max_features=log2, estimator__min_samples_leaf=2, estimator__min_samples_split=2, estimator__n_estimators=300; total time=   6.0s\n",
      "[CV] END estimator__max_depth=10, estimator__max_features=log2, estimator__min_samples_leaf=2, estimator__min_samples_split=2, estimator__n_estimators=300; total time=   6.1s\n",
      "[CV] END estimator__max_depth=50, estimator__max_features=log2, estimator__min_samples_leaf=2, estimator__min_samples_split=5, estimator__n_estimators=300; total time=   6.0s\n",
      "[CV] END estimator__max_depth=50, estimator__max_features=log2, estimator__min_samples_leaf=2, estimator__min_samples_split=5, estimator__n_estimators=300; total time=   5.9s\n",
      "[CV] END estimator__max_depth=50, estimator__max_features=log2, estimator__min_samples_leaf=2, estimator__min_samples_split=5, estimator__n_estimators=300; total time=   5.8s\n",
      "[CV] END estimator__max_depth=50, estimator__max_features=sqrt, estimator__min_samples_leaf=4, estimator__min_samples_split=5, estimator__n_estimators=1000; total time=  30.5s\n",
      "[CV] END estimator__max_depth=50, estimator__max_features=sqrt, estimator__min_samples_leaf=4, estimator__min_samples_split=5, estimator__n_estimators=1000; total time=  30.6s\n",
      "[CV] END estimator__max_depth=50, estimator__max_features=sqrt, estimator__min_samples_leaf=4, estimator__min_samples_split=5, estimator__n_estimators=1000; total time=  30.7s\n",
      "RandomForestRegressor best parameters: {'estimator__n_estimators': 1000, 'estimator__min_samples_split': 2, 'estimator__min_samples_leaf': 4, 'estimator__max_features': 'sqrt', 'estimator__max_depth': 50}\n",
      "Starting RandomizedSearchCV for GradientBoostingRegressor...\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] END estimator__learning_rate=0.2, estimator__max_depth=3, estimator__n_estimators=200; total time= 1.9min\n",
      "[CV] END estimator__learning_rate=0.2, estimator__max_depth=3, estimator__n_estimators=200; total time= 1.9min\n",
      "[CV] END estimator__learning_rate=0.2, estimator__max_depth=3, estimator__n_estimators=200; total time= 1.9min\n",
      "[CV] END estimator__learning_rate=0.05, estimator__max_depth=7, estimator__n_estimators=200; total time= 4.1min\n",
      "[CV] END estimator__learning_rate=0.05, estimator__max_depth=7, estimator__n_estimators=200; total time= 4.1min\n",
      "[CV] END estimator__learning_rate=0.05, estimator__max_depth=7, estimator__n_estimators=200; total time= 4.1min\n",
      "[CV] END estimator__learning_rate=0.2, estimator__max_depth=9, estimator__n_estimators=200; total time= 1.6min\n",
      "[CV] END estimator__learning_rate=0.2, estimator__max_depth=9, estimator__n_estimators=200; total time= 1.6min\n",
      "[CV] END estimator__learning_rate=0.2, estimator__max_depth=9, estimator__n_estimators=200; total time= 2.2min\n",
      "[CV] END estimator__learning_rate=0.1, estimator__max_depth=3, estimator__n_estimators=500; total time= 4.4min\n",
      "[CV] END estimator__learning_rate=0.1, estimator__max_depth=3, estimator__n_estimators=500; total time= 4.5min\n",
      "[CV] END estimator__learning_rate=0.1, estimator__max_depth=3, estimator__n_estimators=500; total time= 4.5min\n",
      "[CV] END estimator__learning_rate=0.05, estimator__max_depth=9, estimator__n_estimators=300; total time= 6.6min\n",
      "[CV] END estimator__learning_rate=0.05, estimator__max_depth=9, estimator__n_estimators=300; total time= 6.6min\n",
      "[CV] END estimator__learning_rate=0.05, estimator__max_depth=9, estimator__n_estimators=300; total time= 6.7min\n",
      "[CV] END estimator__learning_rate=0.3, estimator__max_depth=12, estimator__n_estimators=500; total time= 1.2min\n",
      "[CV] END estimator__learning_rate=0.3, estimator__max_depth=12, estimator__n_estimators=500; total time= 1.2min\n",
      "[CV] END estimator__learning_rate=0.2, estimator__max_depth=9, estimator__n_estimators=500; total time= 1.7min\n",
      "[CV] END estimator__learning_rate=0.1, estimator__max_depth=5, estimator__n_estimators=200; total time= 2.7min\n",
      "[CV] END estimator__learning_rate=0.2, estimator__max_depth=9, estimator__n_estimators=500; total time= 1.7min\n",
      "[CV] END estimator__learning_rate=0.1, estimator__max_depth=5, estimator__n_estimators=200; total time= 2.7min\n",
      "[CV] END estimator__learning_rate=0.1, estimator__max_depth=5, estimator__n_estimators=200; total time= 2.6min\n",
      "[CV] END estimator__learning_rate=0.3, estimator__max_depth=12, estimator__n_estimators=500; total time= 3.6min\n",
      "[CV] END estimator__learning_rate=0.2, estimator__max_depth=9, estimator__n_estimators=500; total time= 3.5min\n",
      "[CV] END estimator__learning_rate=0.01, estimator__max_depth=9, estimator__n_estimators=500; total time=10.2min\n",
      "[CV] END estimator__learning_rate=0.01, estimator__max_depth=9, estimator__n_estimators=500; total time=10.3min\n",
      "[CV] END estimator__learning_rate=0.01, estimator__max_depth=9, estimator__n_estimators=500; total time=10.3min\n",
      "[CV] END estimator__learning_rate=0.01, estimator__max_depth=3, estimator__n_estimators=1000; total time= 5.2min\n",
      "[CV] END estimator__learning_rate=0.01, estimator__max_depth=3, estimator__n_estimators=1000; total time= 5.2min\n",
      "[CV] END estimator__learning_rate=0.01, estimator__max_depth=3, estimator__n_estimators=1000; total time= 4.8min\n",
      "GradientBoostingRegressor best parameters: {'estimator__n_estimators': 1000, 'estimator__max_depth': 3, 'estimator__learning_rate': 0.01}\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Hyperparameter tuning using RandomizedSearchCV\n",
    "rf_search = RandomizedSearchCV(estimator=rf_model, param_distributions=param_grid_rf, n_iter=10, cv=3, n_jobs=-1, random_state=42, verbose=2)\n",
    "gb_search = RandomizedSearchCV(estimator=gb_model, param_distributions=param_grid_gb, n_iter=10, cv=3, n_jobs=-1, random_state=42, verbose=2)\n",
    "\n",
    "# Load training data from disk\n",
    "train_df = pd.read_csv('train_df.csv')\n",
    "X_train = train_df.iloc[:, :-5]\n",
    "y_train = train_df.iloc[:, -5:].values  # Ensure y_train is a 2D array\n",
    "\n",
    "print(\"Starting RandomizedSearchCV for RandomForestRegressor...\")\n",
    "rf_search.fit(X_train, y_train)\n",
    "print(\"RandomForestRegressor best parameters:\", rf_search.best_params_)\n",
    "\n",
    "print(\"Starting RandomizedSearchCV for GradientBoostingRegressor...\")\n",
    "gb_search.fit(X_train, y_train)\n",
    "print(\"GradientBoostingRegressor best parameters:\", gb_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting MultiOutputRegressor for RandomForestRegressor...\n",
      "Fitting MultiOutputRegressor for GradientBoostingRegressor...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultiOutputRegressor(estimator=GradientBoostingRegressor(learning_rate=0.01,\n",
       "                                                         n_estimators=1000,\n",
       "                                                         random_state=42))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultiOutputRegressor</label><div class=\"sk-toggleable__content\"><pre>MultiOutputRegressor(estimator=GradientBoostingRegressor(learning_rate=0.01,\n",
       "                                                         n_estimators=1000,\n",
       "                                                         random_state=42))</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(learning_rate=0.01, n_estimators=1000,\n",
       "                          random_state=42)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(learning_rate=0.01, n_estimators=1000,\n",
       "                          random_state=42)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultiOutputRegressor(estimator=GradientBoostingRegressor(learning_rate=0.01,\n",
       "                                                         n_estimators=1000,\n",
       "                                                         random_state=42))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 10: Update models with best parameters and fit MultiOutputRegressor\n",
    "rf_model = rf_search.best_estimator_\n",
    "gb_model = gb_search.best_estimator_\n",
    "\n",
    "# Initialize MultiOutputRegressor for each base model with best parameters\n",
    "rf_multioutput = rf_model\n",
    "gb_multioutput = gb_model\n",
    "\n",
    "# Fit the MultiOutputRegressor models\n",
    "print(\"Fitting MultiOutputRegressor for RandomForestRegressor...\")\n",
    "rf_multioutput.fit(X_train, y_train)\n",
    "\n",
    "print(\"Fitting MultiOutputRegressor for GradientBoostingRegressor...\")\n",
    "gb_multioutput.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Define Custom Stacking Regressor for MultiOutput\n",
    "class CustomMultiOutputStackingRegressor:\n",
    "    def __init__(self, estimators, final_estimator, cv=5, n_jobs=None):\n",
    "        self.estimators = estimators\n",
    "        self.final_estimator = final_estimator\n",
    "        self.cv = cv\n",
    "        self.n_jobs = n_jobs\n",
    "        self.multi_output_estimators = [MultiOutputRegressor(estimator) for name, estimator in estimators]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.multi_output_estimators_ = [estimator.fit(X, y) for estimator in self.multi_output_estimators]\n",
    "        meta_features = np.column_stack([estimator.predict(X) for estimator in self.multi_output_estimators_])\n",
    "        self.final_estimator_ = MultiOutputRegressor(self.final_estimator).fit(meta_features, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        meta_features = np.column_stack([estimator.predict(X) for estimator in self.multi_output_estimators_])\n",
    "        return self.final_estimator_.predict(meta_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.CustomMultiOutputStackingRegressor at 0x7f84b1787340>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 12: Define and fit the stacking regressor\n",
    "stacking_regressor = CustomMultiOutputStackingRegressor(\n",
    "    estimators=[\n",
    "        ('rf', rf_model.estimator),\n",
    "        ('gb', gb_model.estimator)\n",
    "    ],\n",
    "    final_estimator=RidgeCV(),\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "stacking_regressor.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Regressor Validation MSE: 0.7589455896724522\n",
      "Stacking Regressor Validation R2: 0.3691344890493595\n",
      "Stacking Regressor Validation MAE: 0.610267811175733\n",
      "Trained stacking regressor model saved as 'ensemblev2'.\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Evaluate the stacking regressor\n",
    "X_val = val_df.iloc[:, :-5]\n",
    "y_val = val_df.iloc[:, -5:].values  # Ensure y_val is a 2D array\n",
    "\n",
    "# Ensure the stacking_regressor is fitted\n",
    "if not hasattr(stacking_regressor, 'final_estimator_'):\n",
    "    print(\"Stacking regressor is not fitted. Fit the model before prediction.\")\n",
    "else:\n",
    "    val_predictions = stacking_regressor.predict(X_val)\n",
    "\n",
    "    val_mse = mean_squared_error(y_val, val_predictions, multioutput='uniform_average')\n",
    "    val_r2 = r2_score(y_val, val_predictions, multioutput='uniform_average')\n",
    "    val_mae = mean_absolute_error(y_val, val_predictions, multioutput='uniform_average')\n",
    "    print(f'Stacking Regressor Validation MSE: {val_mse}')\n",
    "    print(f'Stacking Regressor Validation R2: {val_r2}')\n",
    "    print(f'Stacking Regressor Validation MAE: {val_mae}')\n",
    "\n",
    "    # Save the trained stacking regressor\n",
    "    joblib.dump(stacking_regressor, 'ensemblev2.pkl')\n",
    "    print(\"Trained stacking regressor model saved as 'ensemblev2'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/aryan/anaconda3/lib/python3.10/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for NaN values in X_combined and y_combined...\n",
      "NaN values detected. Handling missing values...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot use mean strategy with non-numeric data:\ncould not convert string to float: 'critic declining standards national health service died given large overdose iron hospital doctor read instructions drugs label properly carys pugh 63 former president patients association wales taken casualty royal glamorgan hospital blunder turned skin brown saturated liver iron fought survival hospital seven weeks mrs pugh suffered heart attack contracted deep vein thrombosis legs chest infection ecoli finally suffered second heart attack killed httpwwwtelegraphcouknewsmain26ixhomehtml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m     imputer_y \u001b[38;5;241m=\u001b[39m SimpleImputer(strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     30\u001b[0m     X_combined \u001b[38;5;241m=\u001b[39m imputer_X\u001b[38;5;241m.\u001b[39mfit_transform(X_combined)\n\u001b[0;32m---> 31\u001b[0m     y_combined \u001b[38;5;241m=\u001b[39m \u001b[43mimputer_y\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_combined\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Re-train the model with the combined dataset\u001b[39;00m\n\u001b[1;32m     34\u001b[0m stacking_regressor\u001b[38;5;241m.\u001b[39mfit(X_combined, y_combined)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/base.py:878\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[38;5;66;03m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;66;03m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    877\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/impute/_base.py:390\u001b[0m, in \u001b[0;36mSimpleImputer.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    382\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    383\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m parameter was deprecated in version \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    384\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.1 and will be removed in 1.3. A warning will \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    388\u001b[0m     )\n\u001b[0;32m--> 390\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_fit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;66;03m# default fill_value is 0 for numerical input and \"missing_value\"\u001b[39;00m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;66;03m# otherwise\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfill_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/impute/_base.py:342\u001b[0m, in \u001b[0;36mSimpleImputer._validate_input\u001b[0;34m(self, X, in_fit)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not convert\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(ve):\n\u001b[1;32m    337\u001b[0m     new_ve \u001b[38;5;241m=\u001b[39m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    338\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot use \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m strategy with non-numeric data:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    339\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy, ve\n\u001b[1;32m    340\u001b[0m         )\n\u001b[1;32m    341\u001b[0m     )\n\u001b[0;32m--> 342\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_ve \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ve\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot use mean strategy with non-numeric data:\ncould not convert string to float: 'critic declining standards national health service died given large overdose iron hospital doctor read instructions drugs label properly carys pugh 63 former president patients association wales taken casualty royal glamorgan hospital blunder turned skin brown saturated liver iron fought survival hospital seven weeks mrs pugh suffered heart attack contracted deep vein thrombosis legs chest infection ecoli finally suffered second heart attack killed httpwwwtelegraphcouknewsmain26ixhomehtml'"
     ]
    }
   ],
   "source": [
    "# Cell 14: Pseudo-labeling logic with iterative threshold approach\n",
    "pseudo_labels_threshold = 0.9  # Define a threshold for confident predictions\n",
    "\n",
    "def pseudo_labeling(unlabeled_features, model, threshold):\n",
    "    pseudo_labels = model.predict(unlabeled_features)\n",
    "    confidences = np.max(pseudo_labels, axis=1)\n",
    "    high_confidence_indices = np.where(confidences >= threshold)[0]\n",
    "    return pseudo_labels[high_confidence_indices], high_confidence_indices\n",
    "\n",
    "# Apply pseudo-labeling\n",
    "pseudo_labels, high_confidence_indices = pseudo_labeling(unlabeled_features_df.values, stacking_regressor, pseudo_labels_threshold)\n",
    "\n",
    "# Create a DataFrame with high confidence pseudo-labeled data\n",
    "pseudo_labeled_df = unlabeled_df.iloc[high_confidence_indices].copy()\n",
    "pseudo_labeled_df[['CM', 'AOPV', 'CDACT', 'TI', 'TTBF']] = pseudo_labels\n",
    "\n",
    "# Combine pseudo-labeled data with original labeled data\n",
    "combined_df = pd.concat([labeled_features_df, pseudo_labeled_df])\n",
    "X_combined = combined_df.iloc[:, :-5].values\n",
    "y_combined = combined_df.iloc[:, -5:].values\n",
    "\n",
    "# Check for and handle NaN values\n",
    "print(\"Checking for NaN values in X_combined and y_combined...\")\n",
    "if np.isnan(X_combined).any() or np.isnan(y_combined).any():\n",
    "    print(\"NaN values detected. Handling missing values...\")\n",
    "    # Impute missing values with the mean of the column\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    imputer_X = SimpleImputer(strategy='mean')\n",
    "    imputer_y = SimpleImputer(strategy='mean')\n",
    "    X_combined = imputer_X.fit_transform(X_combined)\n",
    "    y_combined = imputer_y.fit_transform(y_combined)\n",
    "\n",
    "# Re-train the model with the combined dataset\n",
    "stacking_regressor.fit(X_combined, y_combined)\n",
    "\n",
    "# Evaluate the re-trained model\n",
    "val_predictions = stacking_regressor.predict(X_val)\n",
    "\n",
    "val_mse = mean_squared_error(y_val, val_predictions, multioutput='uniform_average')\n",
    "val_r2 = r2_score(y_val, val_predictions, multioutput='uniform_average')\n",
    "val_mae = mean_absolute_error(y_val, val_predictions, multioutput='uniform_average')\n",
    "print(f'Updated Stacking Regressor Validation MSE: {val_mse}')\n",
    "print(f'Updated Stacking Regressor Validation R2: {val_r2}')\n",
    "print(f'Updated Stacking Regressor Validation MAE: {val_mae}')\n",
    "\n",
    "# Save the re-trained stacking regressor\n",
    "joblib.dump(stacking_regressor, 'ensemblev2_retrained.pkl')\n",
    "print(\"Re-trained stacking regressor model saved as 'ensemblev2_retrained'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict hate categories for new messages\n",
    "def predict_hate_categories(model, messages, tokenizer, distilbert_model, tfidf_vectorizer):\n",
    "    tfidf_features = tfidf_vectorizer.transform(messages).toarray()\n",
    "    bert_features = get_bert_features(messages, tokenizer, distilbert_model)\n",
    "    combined_features = np.hstack([tfidf_features, bert_features])\n",
    "    predictions = model.predict(combined_features)\n",
    "    return predictions\n",
    "\n",
    "# Example usage\n",
    "new_messages = [\"I hate people\"]\n",
    "predictions = predict_hate_categories(stacking_regressor, new_messages, tokenizer, distilbert_model, tfidf_vectorizer)\n",
    "print(\"Predictions for new messages:\", predictions)\n",
    "\n",
    "# Print predicted vs actual labels for a sample from the validation set\n",
    "sample_index = np.random.choice(len(X_val), 1)[0]\n",
    "sample_message = val_df.iloc[sample_index, :-5]\n",
    "sample_actual = y_val[sample_index]\n",
    "sample_predicted = val_predictions[sample_index]\n",
    "\n",
    "print(f\"\\nValidation Example:\\nMessage: {labeled_df.iloc[sample_index]['Message']}\")\n",
    "print(f\"Predicted Labels: CM={sample_predicted[0]}, AOPV={sample_predicted[1]}, CDACT={sample_predicted[2]}, TI={sample_predicted[3]}, TTBF={sample_predicted[4]}\")\n",
    "print(f\"Actual Labels: CM={sample_actual[0]}, AOPV={sample_actual[1]}, CDACT={sample_actual[2]}, TI={sample_actual[3]}, TTBF={sample_actual[4]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Define target names\n",
    "target_names = ['CM', 'AOPV', 'CDACT', 'TI', 'TTBF']\n",
    "\n",
    "# Function to plot actual vs predicted values\n",
    "def plot_actual_vs_predicted(y_actual, y_predicted, target_names, title):\n",
    "    fig, axs = plt.subplots(1, y_actual.shape[1], figsize=(20, 5))\n",
    "    fig.suptitle(title)\n",
    "    for i in range(y_actual.shape[1]):\n",
    "        sns.scatterplot(x=y_actual[:, i], y=y_predicted[:, i], ax=axs[i])\n",
    "        axs[i].set_xlabel('Actual')\n",
    "        axs[i].set_ylabel('Predicted')\n",
    "        axs[i].set_title(f'{target_names[i]}')\n",
    "    plt.show()\n",
    "\n",
    "# Function to plot residuals\n",
    "def plot_residuals(y_actual, y_predicted, target_names, title):\n",
    "    fig, axs = plt.subplots(1, y_actual.shape[1], figsize=(20, 5))\n",
    "    fig.suptitle(title)\n",
    "    for i in range(y_actual.shape[1]):\n",
    "        residuals = y_actual[:, i] - y_predicted[:, i]\n",
    "        sns.histplot(residuals, kde=True, ax=axs[i])\n",
    "        axs[i].set_xlabel('Residuals')\n",
    "        axs[i].set_title(f'{target_names[i]}')\n",
    "    plt.show()\n",
    "\n",
    "# Predict on validation set\n",
    "val_predictions = stacking_regressor.predict(X_val)\n",
    "\n",
    "# Visualize actual vs predicted values\n",
    "plot_actual_vs_predicted(y_val, val_predictions, target_names, 'Actual vs Predicted Values')\n",
    "\n",
    "# Visualize residuals\n",
    "plot_residuals(y_val, val_predictions, target_names, 'Residuals Distribution')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
