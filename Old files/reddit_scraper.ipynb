{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import csv\n",
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT_ID = 'nusi2d5hQEOBCnUa1DqwUw'\n",
    "SECRET_KEY = 'RfW2L-070xszCwX7JQtBmsLOXaTfUw'\n",
    "\n",
    "auth = requests.auth.HTTPBasicAuth(CLIENT_ID, SECRET_KEY)\n",
    "\n",
    "# Reading password securely from a file\n",
    "with open('pw.txt', 'r') as f:\n",
    "    pw = f.read().strip()\n",
    "\n",
    "data = {\n",
    "    'grant_type': 'password',\n",
    "    'username': 'AruFanClub',\n",
    "    'password': pw\n",
    "}\n",
    "\n",
    "headers = {'User-Agent': 'MyAPI/0.0.1'}\n",
    "\n",
    "# Obtaining access token from Reddit\n",
    "res = requests.post('https://www.reddit.com/api/v1/access_token', auth=auth, data=data, headers=headers)\n",
    "\n",
    "if 'access_token' in res.json():\n",
    "    TOKEN = res.json()['access_token']\n",
    "else:\n",
    "    print(\"Failed to obtain token. Response was:\", res.json())\n",
    "    raise Exception(\"Failed to obtain token\")\n",
    "\n",
    "# Using the access token directly in the headers for requests made with PRAW\n",
    "headers['Authorization'] = f'bearer {TOKEN}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'User-Agent': 'MyAPI/0.0.1',\n",
       " 'Authorization': 'bearer eyJhbGciOiJSUzI1NiIsImtpZCI6IlNIQTI1NjpzS3dsMnlsV0VtMjVmcXhwTU40cWY4MXE2OWFFdWFyMnpLMUdhVGxjdWNZIiwidHlwIjoiSldUIn0.eyJzdWIiOiJ1c2VyIiwiZXhwIjoxNzEzNDc3MzIyLjk3OTM5NiwiaWF0IjoxNzEzMzkwOTIyLjk3OTM5NiwianRpIjoiNTFRbnphei1sRTFHcnZvSnRNSlJGVnN1anBMSjZ3IiwiY2lkIjoibnVzaTJkNWhRRU9CQ25VYTFEcXdVdyIsImxpZCI6InQyXzdqc3k5cGxkIiwiYWlkIjoidDJfN2pzeTlwbGQiLCJsY2EiOjE1OTY1MTk1NjU5MzAsInNjcCI6ImVKeUtWdEpTaWdVRUFBRF9fd056QVNjIiwiZmxvIjo5fQ.FLwViDOZVC1gtQWz9Zo7HRbTS7dXYDpV2iRrxHs6HQcdb5ub6tFzNX-O693hv0CoJI3o9UFsrlr6uNiLdQtdAv2fAYIiTvsF_wItdWIJXrZFwW7p218eHQ_pBTX-YmjlsXIsGpNAch8G2xcs_JIk2maV838GofHn3ZaYGtaKZdgIdiJDHpOiLePnpG0xBXt7cwVtqaUSuPmtfcoyC4hurCaeU6c-EmFwjjTXbceblMdMptI3JNteePSXuiHilewMgZgxLT3aoYqa77PyQBI0A0JtE6hW-xvDxXUR2D_BFTWPMbXshH-aI7-kcv7mFvne-OyPMagWBsx_QMW46-MRfA'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PRAW with the access token\n",
    "reddit = praw.Reddit(client_id=CLIENT_ID,\n",
    "                     client_secret=SECRET_KEY,\n",
    "                     user_agent='MyAPI/0.0.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for keyword 'hate' written to CSV at: /Users/aryan/Downloads/Joe/controversial_comments_hate.csv\n",
      "Data for keyword 'racist' written to CSV at: /Users/aryan/Downloads/Joe/controversial_comments_racist.csv\n",
      "Data for keyword 'bigot' written to CSV at: /Users/aryan/Downloads/Joe/controversial_comments_bigot.csv\n",
      "Data for keyword 'xenophobia' written to CSV at: /Users/aryan/Downloads/Joe/controversial_comments_xenophobia.csv\n",
      "Data for keyword 'homophobia' written to CSV at: /Users/aryan/Downloads/Joe/controversial_comments_homophobia.csv\n",
      "Data for keyword 'transphobia' written to CSV at: /Users/aryan/Downloads/Joe/controversial_comments_transphobia.csv\n",
      "Data for keyword 'nazi' written to CSV at: /Users/aryan/Downloads/Joe/controversial_comments_nazi.csv\n",
      "Data for keyword 'fascist' written to CSV at: /Users/aryan/Downloads/Joe/controversial_comments_fascist.csv\n",
      "Data for keyword 'supremacist' written to CSV at: /Users/aryan/Downloads/Joe/controversial_comments_supremacist.csv\n",
      "Data for keyword 'incel' written to CSV at: /Users/aryan/Downloads/Joe/controversial_comments_incel.csv\n",
      "Data for keyword 'radical' written to CSV at: /Users/aryan/Downloads/Joe/controversial_comments_radical.csv\n",
      "Data for keyword 'extremist' written to CSV at: /Users/aryan/Downloads/Joe/controversial_comments_extremist.csv\n"
     ]
    }
   ],
   "source": [
    "def fetch_comments_from_controversial_posts(subreddits, keyword, post_limit, comment_limit):\n",
    "    all_comments_collected = []\n",
    "\n",
    "    for subreddit_name in subreddits:\n",
    "        try:\n",
    "            subreddit = reddit.subreddit(subreddit_name)\n",
    "\n",
    "            # Get controversial posts\n",
    "            for post in subreddit.controversial(limit=post_limit):\n",
    "                post_info = {\n",
    "                    'post_id': post.id,\n",
    "                    'post_title': post.title,\n",
    "                    'post_score': post.score,\n",
    "                    'post_url': post.url,\n",
    "                    'poster_username': post.author.name if post.author else '[deleted]'\n",
    "                }\n",
    "                post.comments.replace_more(limit=0)  # Load all comments\n",
    "                for comment in post.comments.list():\n",
    "                    if keyword.lower() in comment.body.lower():\n",
    "                        all_comments_collected.append(\n",
    "                            process_comment(post_info, comment)\n",
    "                        )\n",
    "\n",
    "        except praw.exceptions.PRAWException as e:\n",
    "            print(f\"An error occurred in subreddit {subreddit_name}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # Write results to a single CSV file\n",
    "    csv_file_path = f'controversial_comments_{keyword}.csv'\n",
    "    with open(csv_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\n",
    "            'subreddit', 'post_id', 'post_title', 'post_score', 'poster_username',\n",
    "            'comment_id', 'parent_id', 'commenter_username', 'comment_body',\n",
    "            'comment_score', 'reply_to'\n",
    "        ])\n",
    "        writer.writerows(all_comments_collected)\n",
    "\n",
    "    return os.path.abspath(csv_file_path)\n",
    "\n",
    "def process_comment(post_info, comment):\n",
    "    reply_to = comment.parent_id[3:] if comment.parent_id != comment.link_id else post_info['post_id']\n",
    "    return [\n",
    "        comment.subreddit.display_name,\n",
    "        post_info['post_id'],\n",
    "        post_info['post_title'],\n",
    "        post_info['post_score'],\n",
    "        post_info['poster_username'],\n",
    "        comment.id,\n",
    "        comment.parent_id,\n",
    "        comment.author.name if comment.author else '[deleted]',\n",
    "        comment.body,\n",
    "        comment.score,\n",
    "        reply_to\n",
    "    ]\n",
    "\n",
    "# List of controversial subreddits and keywords\n",
    "subreddits_to_search = ['conspiracy', 'PublicFreakout', 'ChangeMyView', 'politics', 'worldnews', 'news']\n",
    "keywords_to_search = ['hate', 'racist', 'bigot', 'xenophobia', 'homophobia', 'transphobia', 'nazi', 'fascist', 'supremacist', 'incel', 'radical', 'extremist']\n",
    "\n",
    "# Fetch comments from controversial posts in the specified subreddits for each keyword\n",
    "for keyword in keywords_to_search:\n",
    "    csv_output_path = fetch_comments_from_controversial_posts(\n",
    "        subreddits=subreddits_to_search,\n",
    "        keyword=keyword,\n",
    "        post_limit=10,  # Number of controversial posts to fetch from each subreddit\n",
    "        comment_limit=1000  # Maximum number of comments to process from each post\n",
    "    )\n",
    "    print(f\"Data for keyword '{keyword}' written to CSV at: {csv_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asdrp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
